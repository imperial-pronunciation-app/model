{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'phonetic_detail', 'word_detail', 'dialect_region', 'sentence_type', 'speaker_id', 'id'],\n",
       "        num_rows: 4620\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'phonetic_detail', 'word_detail', 'dialect_region', 'sentence_type', 'speaker_id', 'id'],\n",
       "        num_rows: 1680\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "timit = DatasetDict()\n",
    "\n",
    "timit[\"train\"] = load_dataset(\"timit_asr\", data_dir=\"./timit\", split=\"train\")\n",
    "timit[\"test\"] = load_dataset(\"timit_asr\", data_dir=\"./timit\", split=\"test\")\n",
    "\n",
    "timit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/whisper-tiny.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizerFast\n",
    "\n",
    "tokenizer = WhisperTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer/tokenizer.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    # load audio data as 16kHz\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    example[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    example[\"labels\"] = tokenizer(\" \".join(example[\"phonetic_detail\"][\"utterance\"])).input_ids\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_clean = timit.map(prepare_dataset, remove_columns=timit.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sh iy tcl ae dcl d axr dcl d aa r kcl k s ux tcl t ih n gcl r iy s iy epi epi epi ao sh epi epi ao dx er q ao dh iy axr h# h# h# h# h# h# h# h# h# h# h# sh iy epi aa r kcl k epi ax h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# dh ax tcl h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# dh ax tcl s epi epi dh ax tcl s epi epi dh ax tcl h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# dh ax tcl s epi dh ax tcl s epi epi dh ax tcl h# h# h# h# h# h# h# h# h# tcl s epi dh ax tcl s epi epi dh ax tcl s epi epi dh ax tcl s epi epi epi dh ax tcl s epi epi epi dh ax tcl h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# h# tcl h# dh tcl h# h# h# h# h# h# h# h# h# h# h# tcl h# s h# h# h# h# h# h# h# h# h# tcl h# dh ax tcl h# h# h# h# h# h# tcl h# s h# h# h# h# tcl h# s h# dh ax tcl h# h# h# h# h# h# tcl h# s h# dh ax tcl h# h# h# h# h# h# tcl h# s h# dh ax tcl h# h# h# h# h# h# h# h# tcl h# dh ax tcl h# h# h# h# tcl h# s h# h# h# h# tcl h# s h# h# h# tcl h# dh ax tcl h# h# h# h# h# h# h# h# tcl'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features = torch.tensor(timit_clean[\"test\"][0][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "model_trained = WhisperForConditionalGeneration.from_pretrained(\"./openai/whisper-tiny.en-timit/checkpoint-1000\")\n",
    "\n",
    "generated_ids = model_trained.generate(input_features=input_features)\n",
    "\n",
    "transcription = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h# sh iy hv ae dcl d y er dcl d aa r kcl k s uw dx ih ng gcl g r iy s iy w aa sh epi w aa dx er q ao l y iy axr h#'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(timit[\"test\"][0][\"phonetic_detail\"][\"utterance\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
